{
    "lora_rank": 8,
    "lora_dropout": 0.1,
    "modify_layers": ".*q_proj|k_proj|v_proj.*",
    "trainable_param_names": ".*lora_[ab].*",
    "num_train_epochs": 5,
    "learning_rate": 1e-4,
    "train_batch_size": 2,
    "precision": "16",
    "model": "elyza/ELYZA-japanese-Llama-2-7b-instruct",
    "optimizer": "adamw",
    "dataset": "databricks-dolly-15k-ja-osaka",
    "accelerator": "gpu"
}