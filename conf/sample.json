{
    "lora_rank": 8,
    "lora_dropout": 0.1,
    "modify_layers": "query_key_value",
    "trainable_param_names": ".*lora_[qkv]_[ab].*",
    "num_train_epochs": 5,
    "learning_rate": 1e-4,
    "train_batch_size": 32,
    "precision": "bf16",
    "model": "EleutherAI/pythia-70m",
    "optimizer": "adamw"
}